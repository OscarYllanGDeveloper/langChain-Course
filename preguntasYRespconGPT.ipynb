{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1b0237f-6804-4d4f-87ea-9cfc4945dfc3",
   "metadata": {},
   "source": [
    "## Preguntas y Respuestas de Videos de YouTube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f182660-3ed9-497e-b0e1-7d26cf9e8f1c",
   "metadata": {},
   "source": [
    "Code Done by \n",
    "- Developer: Oscar Yllan Garza\n",
    "- GitHub: https://github.com/OscarYllanGDeveloper/langChain-Course\n",
    "- LinkedIn: https://www.linkedin.com/in/oscaryllang/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5636c4c-563f-4dcc-ae64-13033863673b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "833315fe-9b4f-4633-8b54-e1d98d3fbb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b15e440-8148-44d2-b83b-c37a61544b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-dotenv -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac662103-abe4-42f5-a375-9bb0cfc2871b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62e5234f-86d8-426e-9d08-9a31dfaa900a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d689f91b-22a2-4297-b6f2-eac4a934735e",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef62c910-a82b-4564-8a76-fea83880a489",
   "metadata": {},
   "source": [
    "###  YouTube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83b501f5-6080-4c43-9175-6e8ffd12bd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytube  -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21fd8e3c-35c0-4502-85e4-74e5b0ad56bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pytube -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46e70561-2867-4db0-8175-28f020480a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install youtube-transcript-api -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd0224a8-a94d-47b6-b4a1-d97e81774be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-community -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3914fb6f-ff4e-4601-95d2-b0c9abaf9cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import YoutubeLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91b86592-bc09-4684-b6bc-af04792309ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcripción del video en español almacenada en la variable `transcription_text`.\n",
      "sora ha llegado al mundo sorprendiendo a todos porque ahora de repente Es una posibilidad que una Inteligencia artificial pues sea capaz de generar vídeo a través de un único promt de texto logrando resultados como ningún otro modelo de la competencia ha sido capaz de demostrar acostumbrados a ver resultados de pocos segundos con muy poco dinamismo y bastante fallos de coherencia temporal con objetos mutando a cada fotograma que pasa que de repente llegue Open Ai y nos enseñe esto es una demostración de lo muy por delante de la competencia que van en este laboratorio por comparar el modelo más avanzado que teníamos hasta la fecha era el de Google lumier presentado hace un mes y que era bastante espectacular pero claro visto ahora pues ya no lo es Ya que donde uno podía generar clips de 5 segundos ahora hora lo eleva al minuto con una resolución impresionante y una consistencia y comprensión del mundo real que inquieta y justo de esto es de lo que quiero hablar hoy porque nos estamos en centrando mucho en tratar a sora como un generador de vídeo pero es que Open Ai lo que nos cuenta es que sora en realidad es un simulador del mundo real porque fijaos lo que nos han contado de cómo funciona sora es lo siguiente Oye al igual que cuando entrenamos a una ia para que aprenda a generar texto lo primero que tenemos que hacer es convertirlo en trocitos individuales con cierto significado en tokens Pues aquí con las imágenes vamos a hacer lo mismo vamos a descomponer cada fotograma de un vídeo en lo que se llaman visual parches parches visuales en imágenes esto sería subdividir cada imagen en diferentes partes que luego la yaa procesará como otr sitos de información independiente Como sucede con los tokens de texto y claro si trabajamos con vídeo donde tenemos una dimensión temporal más estos parches visuales pues serían bloques tridimensionales que recogerían como una región de una imagen va evolucionando en el tiempo estos bloques de información estos parches visuales será la forma en la que la ia verá y manipulará los los diferentes vídeos que ahora tendríamos que analizar y para analizarlo pues Open Ai ha utilizado aquello que sabe que funciona y en 2024 las dos arquitecturas de moda en el mundo del Deep learning son los Transformers y los modelos de difusión porque ya sabemos que a día de hoy si lo que queremos es calidad de imagen hacer uso de los modelos de difusión es la opción correcta estos modelos son capaces de generar imágenes nuevas a partir de aprender el proceso de filtrar ruido aleatorio es decir podemos un dataset de imágenes con sus descripciones de texto y podemos corromper lasas añadiendo ruido aleatorio para luego entrenar una red neuronal que aprenda a deshacer y filtrar ese ruido pudiendo así llegar de nuevo a la imagen original Claro si la red neuronal aprende a hacer esto para todas estas imágenes Pues luego no va a tener ningún problema en hacerlo con nuevo ruido aleatorio y nuevas descripciones de texto y de ahí es de donde estos modelos pues son capaces de generar nuevas imágenes super realistas modelos como stable diffusion d mid Journey pues se basan en este principio en modelos de difusión y claro lo que pasa es que típicamente la red neuronal que se usa aquí para hacer este proceso de filtrar El ruido es típicamente una red de tipo convolucional y funcionan bien así lo han demostrado pero ya se han encontrado alternativas que parecen funcionar mejor y aquí es donde entra el Transformer y fue hace un año cuando este trabajo de aquí introdujo el concepto de dits diffusion Transformers un modelo de difusión que usa los Transformers para hacer este proceso de filtrado de ruido y un paper que demostraba algo que a Open Yi le encanta y es que al usar Transformers Pues el proceso de difusión de estos diffusion Transformers Pues presentaban mejoras a escala es decir entre más grandes el modelo mejor funcionaba y como digo pocas cosas le gustan más a Open Ai que poder derrochar flops y flops de computación en entrenar una Inteligencia artificial que parece mejorar pues Cuanto más computación le dedicas lo hicieron con el paso de gpt2 a gpt3 y de gpt3 a gpt 4 y ahora lo han hecho consola de hecho fijaos en este ejemplo de aquí como si entrenamos a un diffusion Transformer con una cantidad determinada de computación pues obtenemos un resultado que bueno no está mal pero tampoco está bien Qué pasa si hacemos el modelo más grande y lo entrenamos con más computación Pues que mejora y con cuatro veces más computación parece que el modelo empieza a da resultados espectaculares efectivamente sora Y los diffusion Transformers presentan mejoras a escala y esto es lo poco que sabemos de su entrenamiento lo que han querido compartir sabemos que habrá sido algo similar a lo que acabamos de ver en el uso de diffusion Transformers para imágenes pero en este caso usando vídeos los vídeos como hemos dicho se habrán convertido en una secuencia de parches visuales y a estos parches visuales se le habrá añadido ruido aleatorio a saco el trabajo del Transformer por tanto sería el de aprender a deshacer ese ruido de todos esos parches visuales que por simplificar la explicación voy a representar aquí como fotogramas completos y así el diffusion Transformer irá aprendiendo poco a poco a generar estos parches visuales algo que como hemos visto antes también podemos condicionar a una descripción de lo que va sucediendo en el vídeo van pasando Trozos de vídeo con sus descripciones de texto Trozos de vídeo Trozos de vídeo Trozos de vídeo y el diffusion Transformer poco a poco va aprendiendo a cómo generarlos y con esto Ahora cuando el modelo esté entrenado tras quemar gpus durante semanas al igual que antes pues podremos tomar la descripción que queramos Pues un perrete jugando al ordenador inicializar el modelo con ruido aleatorio y dejar que el Transformer empiece a limpiar ese ruido el resultado Ahora sí será un conjunto de parches visuales que cuando los decodifique nos dará un vídeo que nunca ha existido sora habrá hecho su trabajo y entender esto Mola un montón porque ahora podréis entender qué tan versátil es este esquema de entrenamiento Porque sora no solo se limita a generar vídeos a partir de texto sino que puede hacer muchas más cosas podemos el Transformer colocar una imagen como si fuera un primer fotograma inicializar el resto de fotogramas con ruido y dejar que sora Imagine el resto haciendo que de repente las imágenes pasen a cobrar vida ante nuestros [Música] ojos también con la misma estrategia pues ya podemos extender vídeos en el tiempo hacia delante o hacia detrás solo tendríamos que seleccionar aquellos fotogramas que queramos conservar y dejar luego que sora continúe la película decodificando el resto Esto es lo que nos permite es crear efectos como este de aquí donde podemos tener tres metrajes diferentes cada uno desarrollando la acción de una manera distinta pero que finalmente acaban por converger en un mismo final también podemos usar un primer y un último fotograma que sea idéntico para forzar Así que sora tenga que completar el resto de forma coherente creando por tanto una película que comienza y acaba de la misma forma un bucle infinito y coherente hemos visto también ejemplos de técnicas de image to image donde podemos ir cambiando el estilo de un vídeo pero manteniendo la estructura general de la escena y esto se hace Pues tomando los fotogramas de un vídeo y añadiendo parcialmente ruido a sus fotogramas para darle capacidad a la ia de que complete el resto de información pero manteniendo parte de la estructura original aquí los estilos los podemos ir cambiando según vamos Ajustando el prompt y más espectacular todavía Y esta es una de las cosas que más me huela a la cabeza es la capacidad de sora de poder comenzar la secuencia con los fotogramas de un vídeo y acabar con los fotogramas de un vídeo diferente para dejar así que la Inteligencia artificial se Imagine la película que hace que podamos transicionar de una escena a otra intentando en cualquier caso y haciendo a mi gusto un ejercicio de creatividad excelente que la transición sea lo más coherente posible y de verdad quiero quiero pararme aquí porque esto Es verdaderamente mágico tienes una toma de un dron volando en el Coliseo romano y tienes otra de una mariposa volando bajo el mar y la película que sora inventa para transicionar de una a otra es simplemente magia y si lo pensáis conceptualmente hablando pues lo que estamos viendo aquí es una especie de imp painting pero en este caso aplicado a la dimensión temporal donde es el paso del tiempo el que nos enseña Cómo la Inteligencia artificial ha encontrado la forma de fusionar la información de forma coherente y he viendo estos ejemplos cuando me doy cuenta de que sora no es un modelo que haya memorizado o haya aprendido de una serie de vídeos que existen como tal Y que bueno puede generar variaciones pero poco más No aquí hay que ver a sora como lo que es una Inteligencia artificial que ha aprendido a través de analizar un montón y un montón de vídeos propiedades de cómo funciona el mundo real real propiedades que ahora puede utilizar de forma inteligente y creativa para resolver estos problemas que le estamos planteando y esto amigos es interesante porque esta historia ya la hemos vivido antes y es que recordaréis como el entrenamiento de gpt2 y posteriormente de gpt3 nos enseñó algo fundamental que en su momento fue todo un descubrimiento para el mundo del Deep learning Y es que si tú cogías una Inteligencia artificial y la entrenabas con el único objetivo de aprender a predecir Cuál era el siguiente trozo de texto Pues sí aprendí a predecir cuál era la siguiente palabra generaba texto genial pero también sin ser ese subjetivo la Inteligencia artificial aprendía otras cosas aprendía a traducir textos a resumirlos a responder preguntas a escribir código de programación a escribir poemas un montón de funcionalidades que no era su objetivo principal su objetivo principal era generar texto y como ya he explicado muchas veces Esto fue un bombazo y fue un bombazo que Open Ai quiso explotar y de aquello descubrimientos es de donde salen herramientas que hoy todos utilizamos Como por ejemplo chat gpt Pues fijaos porque ahora con sora podría estar ocurriendo algo similar Y es que el modelo lo único que hace como hemos visto antes es aprender a predecir los siguientes fotogramas ya está y sin embargo por los resultados que nos muestran podemos deducir que el modelo para poder hacer bien su tarea ha tenido que aprender otras tantas habilidades lo que llamaremos habilidades emergentes por ejemplo está claro que el modelo ha tenido que aprender bastante bien a Cómo manejar aspectos de óptica como la reflexión y refracción de la luz en numerosos ejemplos de los que comparten podemos ver reflejos muy realistas iluminaciones naturales coherentes distorsiones más avanzadas como sería el Cómo se distorsiona la luz a través de unas lentes o por ejemplo efectos que vemos cuando estamos sumergidos bajo el agua Esto es algo que sora parece haber aprendido bastante bien y justamente Esto no es algo inédito de este modelo de de hecho gracias a la interactividad en tiempo real de los generadores de imágenes también podemos descubrir que estas capacidades existen en estos modelos fijaos como al mover una mancha blanca a modo de brillo sobre esta escena de globos hace que la Inteligencia artificial infiera un foco de luz y por tanto actualice coherentemente las sombras en respuesta Pero recordemos en ningún momento el objetivo explícito era aprender esto lo único que tenía que aprender era generar imágenes y de forma similar encontramos otras propiedades emergentes por ejemplo la coherencia tridimensional que además en este modelo es espectacular porque al final un vídeo no deja de ser una proyección bidimensional de lo que en una ocasión fue una escena en tres dimensiones y por eso parece que aquí sora no tiene dificultad alguna de mover la cámara y mostrarnos perspectivas coherentes de lo que estamos viendo el modelo entiende la tridimensionalidad del mundo que está creando Y de nuevo esto es impresionante de hecho es tal la consistencia tridimensional de alguna de estas escenas creadas que algunos no han tardado en estos vídeos y procesarlas con técnicas Nerf o the gaus and splin que como ya sabéis nos permiten ahora explorar en tres dimensiones la escena a partir de un texto hemos generado un vídeo que ahora podemos explorar tridimensionalmente más consistencia temporal uno de los grandes problemas de los generadores de vídeo hasta la fecha era esto que los elementos que por un instante estaban en pantalla pues permanecieran ahí de forma consistente cuando le diéramos al Play a mi gusto mucha de esta consistencia temporal en alguno de los modelos más avanzados se consiguió mejorar notablemente pero a Costa de restarle dinamismo a la escena Ja Si dejo parado todos los elementos pues posiblemente todo se mantenga en su sitio sin embargo cuando movíamos las cosas Boom un brazo por aquí que luego no está lo típico y esto parecía una cosa que a la ía se le estaba atragantando pero de nuevo Pues llega open y sora opera a otro nivel aquí no importa que los elementos se muevan que interactúen unos con otros que cambie incluso la toma y de repente la cámara sea un plano cercano de la chica o incluso que haya oclusiones que mantengan tapados a los elementos durante un tiempo las cosas simplemente siguen ahí y esto de nuevo es impresionante porque lo que nos demuestra es que sora sí tiene un concepto de la permanencia espacial y temporal bastante desarrollado y podríamos seguir encontrando más y más ejemplos de habilidades emergentes aprendidas podríamos hablar de las físicas de los tejidos o las dinámicas de fluidos que parece simular con cierta coherencia sistemas de partículas que interactúan con el movimiento de los animales y bueno un conocimiento general de cómo funcionan la mayoría de cosas que genera y es por esto por lo que Open Ai no etiqueta sora solo como un generador de vídeos sino como un simulador del mundo y sí lo sé un simulador del mundo que que no siempre es perfecto partiendo de que aquí Open Ai lo que nos está enseñando es la selección de lo mejor de lo mejor de sora pues podemos estar de acuerdo que no todos los resultados que se generan pues son 100% realistas Y es que por ejemplo por mucho que sora conozca el ciclo de andar de los humanos pues a veces se lía pie derecho pie izquierdo bueno los mezcla también la permanencia de objetos la conoces s pero en ocasiones se olvida de ella y a veces directamente es Matrix lo que falla y la simulación empieza a bugar creando vídeos preocupantemente más interesantes que los vídeos que simulan una realidad coherente no no hace falta que le busquéis tres pies al gato o cinco ya os lo digo yo no sora no es un simulador del Mundo perfecto de hecho es bastante imperfecto pero es impresionante que haya ganado este nivel de comprensión del mundo solo a partir de observar masas de píxeles tridimensionales y Claro todavía desconocemos cuál sería el límite teórico de estos sistema si siguiéramos escalando con más datos Y más computación pero claro aquí la pregunta que habría que hacerse es tiene sentido hacer toda esta inversión en computación para un modelo que solo genera vídeo pues no amigos la cosa va más allá porque lo hemos visto antes con los modelos gpt lo que ha quedado patente Es que su capacidad de entender el texto ha ido tan lejos Que incluso pueden ser capaces de demostrar cierto entendimiento del mundo real y en cierta forma razonar en consecuencia Por ejemplo yo le puedo pedir a chat gpt que razone sobre Cómo ordenar de forma lógica en una pila una serie de objetos con diferentes formas tamaños pesos y él sin haber visto nunca su forma o haber experimentado Cuánto pesa solo con la proyección del mundo que ha podido aprender a partir de leer un montón de texto pues puede razonar una respuesta correcta Su objetivo era leer y sin embargo ha acabado Aprendiendo a cómo razonar sobre el mundo real podemos decir que gpt 4 de alguna forma tiene un entendimiento de Cómo funciona la dinámica del mundo real que ha desarrollado un modelo del mundo o en inglés un World model Pues si lo pensáis ahora sora hace esto pero a lo Bestia y aquí el camino tomado es diferente aquí el aprendizaje No proviene de leer y razonar sobre textos sino que su conocimiento del mundo viene de observarlo visual y temporalmente entiende Como los objetos interactúan entre ellos que las cosas caen que una bombilla en el caparazón de un cangrejo pesa y que debería de arrastrar Por tanto la arena o que un vaso que se rompe en añicos Pues debe de volar en el aire de forma extraña vale su modelo del mundo no es 100% perfecto y aún contiene muchos fallos pero recordemos que es algo e insisto mucho que ha aprendido automáticamente automáticamente en este caso con una visión parcial de lo que sería nuestro mundo solamente viéndolo a través de vídeos pero pensemos qué tan robusto podría ser el modelo del mundo que desarrollara un futuro gpt 6 que sí que se entrenara analizando pues toda esta masa de vídeos y esto lo combinará con su conocimiento de todos los textos que ha leído en internet qué pasaría también si esto lo combinamos con otras modalidades como audio como 3D o como otras fuentes de datos más avanzadas Qué tipo de representación interna podría desarrollar un modelo de estas características y bueno para qué no serviría pues fijaos ejemplo cotidiano el otro día en casa sandr no encontraba unos auriculares que había dejado en el baño sí habíamos encontrado uno de ellos en el suelo y ella buscando y buscando me decía que no encontraba el otro hasta que me puse a pensar qué podría haber pasado y entonces Ja abrí el cajón y lo encontré ahí dentro claro para llegar a esa conclusión internamente en mi cabeza lo que reproduje fue lo que podía haber pasado si un auricular estaba en el suelo es que había rodado y había caído desde arriba y si el otro no estaba en el suelo que lo habíamos buscado bastante es que tenía que haber caído en otro lado y en el baño habitualmente los cajones suelen estar cerrados pero también a veces abiertos Así que había cierta probabilidad de que hubiera caído en su interior cuadrando bastante con que no lo estuviéramos encontrando en el suelo claro para yo poder hacer esta deducción que podríamos etiquetar de inteligente pues en mi cabeza tuve que Reproducir todas estas dinámicas necesitaba contar en mi cabeza con un modelo del mundo en el que poder simular ciertas físicas ciertas dinámicas y que no tiene que ser perfecto en mi cabeza yo no estaba calculando que la caída de los objetos fuera 9,8 m por segundo pero la simulación era lo suficientemente buena como para poder proponer una hipótesis de valor y ahí está la clave lo que está demostrando sora aquí es su capacidad de modelar parte del mundo y Esto va a tener una utilidad directa en futuros modelos ya sean chatbots virtuales que nos hablen a través de texto y que podrán hacer deducciones e inferencias mucho más avanzadas o robots que directamente aprenderán razonar Ann y actuarán aplicando su conocimiento del mundo real y es por esto por lo que Open Ai nos habla de simulador del mundo y no de generador de vídeos y aquí es donde se abre el debate Porque muchos estaréis de acuerdo en que la coincidencia de sora saliendo el mismo día que gemini 1.5 Pues sí pudo perjudicar a Google pero realmente al que le tuvo que sentar como una patada en las gpus fue al bueno de Jan le Kun porque ese día el director científico de Inteligencia artificial en meta tenía preparada una gran Release su modelo v yepa v de vídeo y yepa que son las siglas del proyecto que lleva desde hace años desarrollando y que para él es su gran apuesta sobre el futuro de la inteligencia artificial en este caso un modelo que a través del análisis de vídeo pues aplicando estrategias muy diferentes a sora es capaz de aprender a entender el mundo es decir a conformar su propio World model y por esto creo que al bueno de Jan le Kun le tienea que haber bueno molestado el haber sido eclipsado por un modelo como sora que compite más o menos en esta línea bueno por eso y además porque recordáis los diffusion Transformers que hacen funcionar a sora y que comentábamos antes de un artículo de hace un año Bueno ese artículo provenía de los laboratorios de meta fatal y quizás un poquito por eso hemos visto a Jan Pues en Twitter criticando lo que sora aporta en materia de creación de modelos del mundo pero la crítica en parte es válida porque lo que indica Es que para poder considerar a sora un simulador del mundo no solo basta con renderizar un vídeo realista y espectacular del mundo no se tiene que poder interactuar con él y para que entendáis bien a lo que me refiero quiero que veáis este último ejemplo generado por sora esto de aquí es el resultado generado ante el prompt Minecraft un vídeo en el que si no te fijas mucho los detalles pues puede pasar fácil por un Gameplay Real del juego vamos a ignorar a esta vaca cerdo que nos está mirando con ojitos raros claro Qué significa esto esto significa que sora entiende el mundo de Minecraft de una forma tan general que incluso lo puede simular o solo estás copiando Gameplay que ha aprendido tras ver un montón de vídeos y ya pues una forma de Saber esto de poder evaluar la capacidad de generalizar de este simulador de de mundos es interactuando con él jugándolo es decir podríamos entrenar a sora para que no tome como input solo un prom de texto sino que también pudiera tomar acciones de un teclado de un ratón y bueno si nos demostrara que podemos ejecutar acciones sobre este vídeo generado y el modelo respondiera Consecuentemente generando el vídeo de respuesta ante esa acción Entonces sí estaríamos ante un simulador que ha aprendido un modelo del mundo válido y esto de tenerlo es potentísimo y hablaremos de ello en un duro vídeo dedicado a World models a modelos del mundo y a Epa porque es un tema que trae pues una tecnología que es muy interesante y muy prometedora hacia un futuro de inteligencias artificiales más generales y más capaces Pero para que no os vayáis sin un ejemplo de a lo que me refiero quiero que veáis este último paper salido en los últimos días un modelo de Google llamado G que consiste en un modelo generativo que lo que te va a permitir es generar en segundos lo que parece ser un juego simulado un juego simulado por una aa gener relativa que ha aprendido a partir de vídeos y que Oh podemos interactuar con él Algo que tal y como indican ellos en el paper se trataría de un modelo fundacional capaz de crear modelos del mundo por tanto las conclusiones con las que quiero que os quedéis tras Este vídeo son las siguientes bueno eh aprovecho un momento para recordaros que el gtc el gran evento de nvidia está a la vuelta de la esquina y que como en años anteriores estamos sorteando una tarjeta gráfica que puede ser tuya yo este año al ev voy a asistir en persona viajo a silicon Valley pero vosotros podéis asistir virtualmente y además de ver un montón de ponencias Super interesantes podréis optar por esta gpu os dejo todos los detalles abajo en el comentario fijado y ahora sí las conclusiones primero técnicamente Open Ai está muy muy por delante de la competencia al menos así lo han demostrado con este trabajo de aquí y de ellos pues ya me espero cualquier cosa que puedan sacar en los próximos meses nos han presentado una tecnología que es impresionante pero que viene con un montón de debates éticos con transformaciones de un montón de industrias y también un montón de oportunidades segundo con sora lo que Open aa ha demostrado es la efectividad de entrenar a los diffusion Transformers a escala una tecnología que más allá de utilizarse para generar vídeos también va a tener un montón de aplicaciones en otros contextos en otros Campos en otras áreas en otros problemas del mundo del Deep learning no nos quedemos con que esto Solo es un generador de vídeo es algo mucho más más amplio y transversal y tercero que Open Ai Como hizo ya en el pasado con los modelos de generación de texto también ha demostrado que existen ciertas habilidades emergentes aprendidas que hacen de estos modelos no solo un generador de vídeos sino simuladores de mundos simuladores de mundos imperfectos pero que actualmente son capaces de enseñarnos visiones del mundo hiperrealistas y que no nos quepa duda que Open Ai\n"
     ]
    }
   ],
   "source": [
    "from pytube import YouTube\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "def get_video_id(url):\n",
    "    \"\"\"Extrae el ID del video de YouTube usando Pytube.\"\"\"\n",
    "    try:\n",
    "        yt = YouTube(url)\n",
    "        return yt.video_id\n",
    "    except Exception as e:\n",
    "        print(f\"Error al obtener el ID del video: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_transcript(video_url, language=\"es\"):\n",
    "    \"\"\"Obtiene la transcripción en español de un video de YouTube usando su URL.\"\"\"\n",
    "    video_id = get_video_id(video_url)\n",
    "    if not video_id:\n",
    "        print(\"URL no válida o no se pudo obtener el ID del video.\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=[language])\n",
    "        transcription_text = \" \".join([entry[\"text\"] for entry in transcript])\n",
    "        return transcription_text\n",
    "    except Exception as e:\n",
    "        print(f\"No se pudo obtener la transcripción en español: {e}\")\n",
    "        return None\n",
    "\n",
    "# Ejemplo de uso\n",
    "video_url = \"https://www.youtube.com/watch?v=nubNmOQLgxQ\"\n",
    "transcription_text = get_transcript(video_url)\n",
    "\n",
    "if transcription_text:\n",
    "    print(\"Transcripción del video en español almacenada en la variable `transcription_text`.\")\n",
    "    print(transcription_text)\n",
    "else:\n",
    "    print(\"No se encontró transcripción en español para este video.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad310d26-ebcb-4c72-9f80-c47341da953e",
   "metadata": {},
   "source": [
    "## Paso 2: Configurar LangChain e Instalar las Bibliotecas Necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28b57583-a023-49eb-a347-92a00f860db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /opt/anaconda3/lib/python3.11/site-packages (0.3.7)\n",
      "Requirement already satisfied: openai in /opt/anaconda3/lib/python3.11/site-packages (1.52.2)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.9.0-cp311-cp311-macosx_10_14_x86_64.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/lib/python3.11/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/anaconda3/lib/python3.11/site-packages (from langchain) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/anaconda3/lib/python3.11/site-packages (from langchain) (3.10.10)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /opt/anaconda3/lib/python3.11/site-packages (from langchain) (0.3.15)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from langchain) (0.3.0)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /opt/anaconda3/lib/python3.11/site-packages (from langchain) (0.1.137)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/anaconda3/lib/python3.11/site-packages (from langchain) (1.24.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/anaconda3/lib/python3.11/site-packages (from langchain) (2.9.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/lib/python3.11/site-packages (from openai) (4.6.2.post1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.11/site-packages (from openai) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.11/site-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/lib/python3.11/site-packages (from openai) (0.6.1)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/lib/python3.11/site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/anaconda3/lib/python3.11/site-packages (from openai) (4.12.2)\n",
      "Collecting numpy<2,>=1 (from langchain)\n",
      "  Using cached numpy-1.26.4-cp311-cp311-macosx_10_9_x86_64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.11/site-packages (from faiss-cpu) (23.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.16.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (1.33)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/anaconda3/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.10)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /opt/anaconda3/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain) (3.0.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
      "Downloading faiss_cpu-1.9.0-cp311-cp311-macosx_10_14_x86_64.whl (7.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached numpy-1.26.4-cp311-cp311-macosx_10_9_x86_64.whl (20.6 MB)\n",
      "Installing collected packages: numpy, faiss-cpu\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.4\n",
      "    Uninstalling numpy-1.24.4:\n",
      "      Successfully uninstalled numpy-1.24.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pyfume 0.3.4 requires numpy==1.24.4, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed faiss-cpu-1.9.0 numpy-1.26.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain openai faiss-cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6c3f6c7-5916-475d-9cea-b5ad3c8ed7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai.api_key\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a923b409-1027-4eea-a9d5-d1596f76a3b5",
   "metadata": {},
   "source": [
    "## Paso 3: Dividir la Transcripción en Fragmentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb7ebeb5-1c49-4a4c-978d-4753640ad0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# Dividir la transcripción en fragmentos de 1000 caracteres (ajustable)\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "texts = text_splitter.split_text(transcription_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75da36b-9a82-45cf-8cda-72d78470cf6d",
   "metadata": {},
   "source": [
    "## Paso 4: Crear Embeddings e Indexar con FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ec2353e-18aa-45eb-983d-52fceebcd0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Crear embeddings de OpenAI\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Crear la base de datos FAISS con los textos divididos\n",
    "docsearch = FAISS.from_texts(texts, embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c49557-89d9-441a-87db-293e9e016ea1",
   "metadata": {},
   "source": [
    "## Paso 5: Hacer Preguntas a la Transcripción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95e871ff-25ed-45ec-86c4-d663eac19a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5n/grc0_q650jx0vr0f15rnkbh00000gn/T/ipykernel_3384/3882096991.py:5: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(model=\"gpt-3.5-turbo\")  # o usa \"gpt-4\" si tienes acceso\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El video trata principalmente sobre el modelo de inteligencia artificial llamado \"sora\", desarrollado por Open Ai, que es capaz de generar vídeos realistas a partir de texto. Se discute cómo sora no solo es un generador de vídeo, sino un simulador del mundo real que ha aprendido a través de analizar una gran cantidad de vídeos. También se menciona la capacidad de sora para modelar el mundo y la importancia de la interactividad para evaluar su capacidad de generalización. Además, se discuten las implicaciones éticas y las oportunidades que esta tecnología presenta.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "# Configurar el modelo de chat con `ChatOpenAI`\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")  # o usa \"gpt-4\" si tienes acceso\n",
    "\n",
    "# Crear una cadena de preguntas y respuestas\n",
    "qa_chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "\n",
    "# Hacer una pregunta\n",
    "query = \"¿De qué trata principalmente el video?\"\n",
    "docs = docsearch.similarity_search(query)\n",
    "\n",
    "# Obtener la respuesta\n",
    "answer = qa_chain.run(input_documents=docs, question=query)\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08b53c4-0670-4474-8d3d-09c1917f720b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
